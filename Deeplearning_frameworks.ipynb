{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfsPscoqLjzb"
      },
      "outputs": [],
      "source": [
        "#Q1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x?\n",
        "\"\"\"TensorFlow 2.0 is a major update to TensorFlow, Google’s deep learning framework. The key differences between TensorFlow 2.x and TensorFlow 1.x include:\n",
        "\n",
        "Eager Execution: TensorFlow 2.0 enables eager execution by default, which allows for more intuitive, dynamic, and interactive model building and debugging, whereas TensorFlow 1.x used static computation graphs.\n",
        "Simplified APIs: TensorFlow 2.0 has a much cleaner and simplified API. Many redundant or complicated functions from 1.x were removed or merged into more intuitive structures.\n",
        "Keras Integration: TensorFlow 2.0 has Keras as its high-level API for building and training models, making it easier to work with.\n",
        "Improved Performance: TensorFlow 2.0 brings better performance and support for distributed training, multi-GPU training, and easier model deployment.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. How do you install TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "Cs6Ao4k_STEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn7QtD05SZNm",
        "outputId": "5df63c4f-0345-46d0-93d1-5f8ec373504e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. What is the primary function of tf.function in TensorFlow 2.0?\n",
        "\"\"\"tf.function is a decorator in TensorFlow 2.0 that turns Python functions into TensorFlow graphs. The key benefits are:\n",
        "\n",
        "Performance Optimization: By converting Python functions to TensorFlow graphs, tf.function can accelerate performance by utilizing optimized graph-based execution (via TensorFlow's XLA compiler).\n",
        "Cleaner Code: It enables you to write code that is both easy to debug (eager execution) and optimized for production (graph execution).\"\"\""
      ],
      "metadata": {
        "id": "ykhC5MBWSck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def add(a, b):\n",
        "    return a + b\n"
      ],
      "metadata": {
        "id": "JgniAc6HSo50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. What is the purpose of the Model class in TensorFlow 2.0?\n",
        "\"\"\"The Model class in TensorFlow 2.0 (via Keras) represents the architecture of your neural network. It defines how data flows through the layers, and it handles tasks like:\n",
        "\n",
        "Building layers and organizing them.\n",
        "Compiling the model with an optimizer, loss function, and metrics.\n",
        "Training the model on data.\n",
        "Evaluating the model on unseen data.\n",
        "There are two primary ways to create models in TensorFlow 2.0:\n",
        "\n",
        "Sequential API: For simple linear stack of layers.\n",
        "Functional API: For more complex models, like those with multiple inputs or outputs.\"\"\""
      ],
      "metadata": {
        "id": "xf5yFOvoSndS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "inputs = Input(shape=(32,))\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "VCH2gJ3xS0wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. How do you create a neural network using TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "srPKn5scS6ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers\n",
        "model.add(Dense(64, activation='relu', input_shape=(784,)))  # Input layer and first hidden layer\n",
        "model.add(Dense(64, activation='relu'))  # Second hidden layer\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n"
      ],
      "metadata": {
        "id": "d9YEIaLWS-EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. What is the importance of Tensor Space in TensorFlow?\n",
        "\"\"\"Tensor space refers to the multi-dimensional arrays (or tensors) that are the core data structure in TensorFlow. Each tensor has a rank (number of dimensions) and a shape (the size of each dimension). TensorFlow operations manipulate these tensors, and most deep learning algorithms use tensors to represent input data, model parameters (weights), and intermediate results.\"\"\""
      ],
      "metadata": {
        "id": "TCaQE1TMTAFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. How can TensorBoard be integrated with TensorFlow 2.0?\n",
        "\"\"\"TensorBoard is a visualization tool that helps you monitor and debug your training process. You can integrate it into TensorFlow 2.0 as follows:\n",
        "\n",
        "Create a callback to log data.\n",
        "Use tensorboard callback during training.\"\"\""
      ],
      "metadata": {
        "id": "BCOrNxuzTHdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# Define the log directory\n",
        "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
        "\n",
        "# Train the model with the callback\n",
        "model.fit(x_train, y_train, epochs=5, callbacks=[tensorboard_callback])\n"
      ],
      "metadata": {
        "id": "q6_gfo04THRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. What is the purpose of TensorFlow Playground?\n",
        "\"\"\"TensorFlow Playground is an interactive tool that allows you to experiment with neural networks in a web-based interface. It’s great for visualizing how changing hyperparameters like learning rate, number of layers, and activation functions affect the training of a neural network on toy datasets. It’s useful for beginners to understand neural networks and deep learning concepts in a hands-on way.\n",
        "\n",
        "You can access it here: TensorFlow Playground\"\"\""
      ],
      "metadata": {
        "id": "10A2M7qVTOrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. What is Netron, and how is it useful for deep learning models?\n",
        "\"\"\"Netron is a tool for visualizing deep learning models. It supports many different model formats, such as TensorFlow, Keras, PyTorch, ONNX, and more. Netron allows you to inspect the architecture of trained models, see layer types, input/output shapes, and other relevant details, which is useful for model debugging and understanding.\n",
        "\n",
        "You can use Netron online or install it locally: Netron website\"\"\""
      ],
      "metadata": {
        "id": "sy0KYjkvTPD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10. What is the difference between TensorFlow and PyTorch?\n",
        "\"\"\"TensorFlow and PyTorch are both popular deep learning frameworks, but they have some differences:\n",
        "\n",
        "Dynamic vs Static Graph: PyTorch uses dynamic computation graphs (eager execution by default), whereas TensorFlow 1.x uses static graphs (though TensorFlow 2.0 uses eager execution as well).\n",
        "API Style: PyTorch’s API is more Pythonic and closely resembles NumPy, while TensorFlow is more structured and suited for production-level deployment.\n",
        "Deployment: TensorFlow provides robust tools for model deployment (e.g., TensorFlow Serving, TensorFlow Lite, TensorFlow.js), while PyTorch's deployment tools are not as mature (but improving with tools like TorchServe and ONNX).\"\"\""
      ],
      "metadata": {
        "id": "r1zHqlyGTG-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11. How do you install PyTorch?"
      ],
      "metadata": {
        "id": "Qy2lZk0RTjof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiK6dY5RTn9B",
        "outputId": "b7baa0df-2350-4f9f-9905-a8d2439261be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12. What is the basic structure of a PyTorch neural network?"
      ],
      "metadata": {
        "id": "VeMDS5zOTwu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(784, 128)\n",
        "        self.layer2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = SimpleNN()\n"
      ],
      "metadata": {
        "id": "yQmxgK6sTytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13. What is the significance of tensors in PyTorch?\n",
        "\"\"\"Tensors in PyTorch are multi-dimensional arrays similar to NumPy arrays, but they are optimized for GPU operations. Tensors are the primary data structure in PyTorch, representing inputs, outputs, and model parameters. They support automatic differentiation (autograd), which is crucial for backpropagation in training neural networks.\"\"\""
      ],
      "metadata": {
        "id": "COFZci4tTyp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch?\n",
        "\"\"\"torch.Tensor: A standard tensor that resides in CPU memory.\n",
        "torch.cuda.Tensor: A tensor that resides on the GPU, enabling faster computation using CUDA (NVIDIA GPUs). You can move a tensor to the GPU using .to('cuda') or .cuda().\"\"\""
      ],
      "metadata": {
        "id": "kEi1o_hITymg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2)  # Tensor on CPU\n",
        "x_gpu = x.to('cuda')   # Move tensor to GPU\n"
      ],
      "metadata": {
        "id": "u2tNvxDATyhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15. What is the purpose of the torch.optim module in PyTorch?\n",
        "\"\"\"The torch.optim module contains optimizers used to update model parameters (weights) during training. PyTorch provides a variety of optimizers such as:\n",
        "\n",
        "SGD (Stochastic Gradient Descent)\n",
        "Adam\n",
        "RMSProp\n",
        "These optimizers help minimize the loss function by adjusting the weights based on the gradients computed during backpropagation.\"\"\""
      ],
      "metadata": {
        "id": "llJocUQmTyY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create a model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop example\n",
        "optimizer.zero_grad()  # Zero gradients\n",
        "output = model(input)  # Forward pass\n",
        "loss = loss_fn(output, target)  # Calculate loss\n",
        "loss.backward()  # Backward pass\n",
        "optimizer.step()  # Update weights\n"
      ],
      "metadata": {
        "id": "95lD7yETTyKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q16. What are some common activation functions used in neural networks?\n",
        "Activation functions play a crucial role in determining the output of a neural network layer. Here are some commonly used activation functions:\n",
        "\n",
        "\n",
        "**Sigmoid (Logistic Function)**\n",
        "\n",
        "Formula: 1 / (1 + e^(-x))\n",
        "\n",
        "Output Range: 0 to 1\n",
        "\n",
        "Commonly used for binary classification.\n",
        "\n",
        "Problem: Can cause vanishing gradients.\n",
        "\n",
        "**Tanh (Hyperbolic Tangent)**\n",
        "\n",
        "Formula: (e^x - e^(-x)) / (e^x + e^(-x))\n",
        "\n",
        "Output Range: -1 to 1\n",
        "\n",
        "Better than sigmoid because it is zero-centered.\n",
        "\n",
        "Still has vanishing gradient issues.\n",
        "\n",
        "**ReLU (Rectified Linear Unit)**\n",
        "\n",
        "Formula: max(0, x)\n",
        "\n",
        "Output Range: 0 to ∞\n",
        "\n",
        "Most commonly used in hidden layers.\n",
        "\n",
        "Fast and efficient but can lead to \"dying neurons\".\n",
        "\n",
        "**Leaky ReLU**\n",
        "\n",
        "Formula: x if x > 0, else αx (α is a small constant like 0.01)\n",
        "\n",
        "Fixes the problem of dying ReLU by allowing small gradient when x < 0.\n",
        "\n",
        "**Softmax**\n",
        "\n",
        "Used in the output layer for multi-class classification.\n",
        "\n",
        "Converts outputs into probabilities that sum to 1."
      ],
      "metadata": {
        "id": "NrwK98nlmhGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch?\n",
        "torch.nn.Module: This is the base class for all neural network modules in PyTorch. You can subclass Module to create custom models. You are free to define the layers and how data flows through the network in the forward method."
      ],
      "metadata": {
        "id": "eVU51vgoUcJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "FuVcytjAUb_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n"
      ],
      "metadata": {
        "id": "s9AkDR14UrcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18, How can you monitor training progress in TensorFlow 2.0?\n",
        "In TensorFlow 2.0, you can monitor training progress by:\n",
        "\n",
        "Using TensorBoard: You can log data such as loss and accuracy for each epoch, and visualize these metrics in TensorBoard."
      ],
      "metadata": {
        "id": "qho-VCjbUrwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# Create a TensorBoard callback\n",
        "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
        "\n",
        "# Train the model with the callback\n",
        "model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n"
      ],
      "metadata": {
        "id": "ezlYBNeXUsDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3-b5LF53Ub0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q19. How does the Keras API fit into TensorFlow 2.0?\n",
        "In TensorFlow 2.0, Keras is the default high-level API for building and training neural networks. It provides a simple and intuitive way to define, compile, and train deep learning models. Keras in TensorFlow 2.0 has been fully integrated and supports:\n",
        "\n",
        "Model building (with the Sequential and Functional APIs)\n",
        "Loss functions, optimizers, and metrics\n",
        "Model training and evaluation (via model.fit(), model.evaluate(), etc.)\n",
        "Callbacks for monitoring training and fine-tuning models\n",
        "Keras abstracts away much of the complexity of TensorFlow while still providing access to the full power of TensorFlow when needed. This makes it easier for both beginners and advanced users to develop models."
      ],
      "metadata": {
        "id": "xTELw1vQUbqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q20. What is an example of a deep learning project that can be implemented using TensorFlow 2.0?\n",
        "One great example of a deep learning project you can implement using TensorFlow 2.0 is image classification with a convolutional neural network (CNN). Here’s a simplified version of how you might build it:\n",
        "\n",
        "Download a dataset (e.g., CIFAR-10, MNIST, etc.)\n",
        "Preprocess the data (e.g., normalize, reshape, etc.)\n",
        "Define a CNN model in TensorFlow 2.0 using the Keras API.\n",
        "Train the model on the dataset.\n",
        "Evaluate the model's performance."
      ],
      "metadata": {
        "id": "JO1HJ-FfUbdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "t6cEjjidVADg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Practical"
      ],
      "metadata": {
        "id": "dbda9mtcVAqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1. How do you install and verify that TensorFlow 2.0 was installed successfully?"
      ],
      "metadata": {
        "id": "y8QAU2lxVeri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HndYmYOHVg3T",
        "outputId": "5d7f4033-3b6f-4ad5-8a51-786b83aeb176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. How can you define a simple function in TensorFlow 2.0 to perform addition?"
      ],
      "metadata": {
        "id": "veF_xLGzVjss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the function\n",
        "def add(a, b):\n",
        "    return tf.add(a, b)\n",
        "\n",
        "# Example usage\n",
        "a = tf.constant(5)\n",
        "b = tf.constant(3)\n",
        "result = add(a, b)\n",
        "print(\"Result:\", result.numpy())  # Output: 8\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMsBXvkjVnTw",
        "outputId": "2ba5a5dd-9259-4875-be2b-963137652b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. How can you create a simple neural network in TensorFlow 2.0 with one hidden layer?"
      ],
      "metadata": {
        "id": "Kv_ZPvWEVr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Create a simple neural network\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(784,)),  # Hidden layer with 64 units\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 units (for classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Model summary to see the architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "392RxNqdVvn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. How can you visualize the training progress using TensorFlow and Matplotlib?"
      ],
      "metadata": {
        "id": "GeFBFQ6iVzTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# Train the model and store the history\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Plot the loss and accuracy curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1LMQIi-yVzIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. How do you install PyTorch and verify the PyTorch installation?"
      ],
      "metadata": {
        "id": "G0uMJ3e-V5Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "# Check if CUDA is available (for GPU)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8h96X_yVv7n",
        "outputId": "0b93cd3e-ab81-4e63-8f77-93eec8234eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. How do you create a simple neural network in PyTorch?"
      ],
      "metadata": {
        "id": "is10bAosVwKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)  # Hidden layer\n",
        "        self.fc2 = nn.Linear(64, 10)   # Output layer (10 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)              # Output layer\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "ww4kiQiUWEIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. How do you define a loss function and optimizer in PyTorch?"
      ],
      "metadata": {
        "id": "8cb_FigkVwaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define a loss function (e.g., Cross-Entropy Loss for classification)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an optimizer (e.g., Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example training loop\n",
        "optimizer.zero_grad()  # Zero the gradients\n",
        "output = model(x_train)  # Forward pass\n",
        "loss = loss_function(output, y_train)  # Compute loss\n",
        "loss.backward()  # Backward pass (compute gradients)\n",
        "optimizer.step()  # Update the weights\n"
      ],
      "metadata": {
        "id": "Ku68A8bGWKsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. How do you implement a custom loss function in PyTorch?"
      ],
      "metadata": {
        "id": "N-Oa-EGnWKjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        # Example: Mean Squared Error (MSE) loss\n",
        "        return torch.mean((output - target) ** 2)\n",
        "\n",
        "# Instantiate the custom loss function\n",
        "loss_function = CustomLoss()\n",
        "\n",
        "# Use it in training\n",
        "output = model(x_train)\n",
        "loss = loss_function(output, y_train)\n",
        "loss.backward()\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "TcRqb507WKYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. How do you save and load a TensorFlow model?"
      ],
      "metadata": {
        "id": "-h8VYjHEWUNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.h5') #saving a model\n"
      ],
      "metadata": {
        "id": "xdCYs6ixWY-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "loaded_model = load_model('my_model.h5') #loading a model\n"
      ],
      "metadata": {
        "id": "b4nQhCZjWY0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CD-fDJk7Wi8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}