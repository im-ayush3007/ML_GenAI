{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiW2_bHkifDl"
      },
      "outputs": [],
      "source": [
        "### 1. **What is the main purpose of RCNN in object detection?**\n",
        "\n",
        "**RCNN (Region-based Convolutional Neural Networks)** is designed to perform object detection by combining the power of CNNs with region proposals. The main idea is to use a selective search algorithm to propose candidate regions in an image where objects are likely to be found, then apply a CNN to classify these regions and refine the bounding boxes.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **What is the difference between Fast RCNN and Faster RCNN?**\n",
        "\n",
        "- **Fast RCNN**: It improves upon the original RCNN by eliminating the need to extract features for each region proposal. Instead, it extracts features for the entire image and then performs RoI (Region of Interest) pooling to generate fixed-size feature maps for each proposal.\n",
        "\n",
        "- **Faster RCNN**: It takes Fast RCNN further by introducing a Region Proposal Network (RPN), which eliminates the need for external region proposal methods like selective search. RPN generates region proposals directly from the feature map, speeding up the process significantly.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **How does YOLO handle object detection in real-time?**\n",
        "\n",
        "**YOLO (You Only Look Once)** treats object detection as a single regression problem. Instead of proposing regions and then classifying them (like in R-CNN), YOLO divides the image into a grid and predicts bounding boxes and class probabilities directly from the entire image in one forward pass. This leads to fast processing, making it suitable for real-time object detection.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Explain the concept of Region Proposal Networks (RPN) in Faster RCNN.**\n",
        "\n",
        "**Region Proposal Networks (RPN)** are a key innovation in Faster RCNN. RPNs are designed to generate high-quality region proposals from the feature map of an image. Unlike selective search in previous versions, RPNs use convolutional layers to predict bounding box coordinates and scores for potential objects. These proposals are then passed to the Fast RCNN for further classification and refinement.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **How does YOLOv9 improve upon its predecessors?**\n",
        "\n",
        "While YOLOv9 doesn't officially exist as of now, if we consider advancements made in the previous versions like YOLOv4 and YOLOv5, improvements have been made in:\n",
        "- **Speed**: YOLOv9 could enhance speed by reducing computational overhead, optimizing architecture, and using more efficient backbones.\n",
        "- **Accuracy**: Using better backbone networks, more anchor boxes, and advanced feature extraction techniques to improve detection accuracy.\n",
        "- **Real-time Performance**: Improvements in model pruning and quantization could make YOLOv9 even more suitable for real-time applications.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **What role does non-max suppression play in YOLO object detection?**\n",
        "\n",
        "**Non-Maximum Suppression (NMS)** is used to eliminate redundant bounding boxes. After YOLO predicts multiple bounding boxes for the same object, NMS removes the overlapping boxes that have a lower confidence score, leaving only the box with the highest confidence to represent the detected object.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Describe the data preparation process for training YOLOv9.**\n",
        "\n",
        "Data preparation for training YOLO typically involves:\n",
        "1. **Annotation**: Labeling images with bounding boxes and object class labels.\n",
        "2. **Preprocessing**: Resizing images to a standard size, normalization (scaling pixel values), and possibly augmenting the data (flipping, rotation, scaling) to increase the dataset's diversity.\n",
        "3. **Anchor Box Design**: Selecting suitable anchor box sizes that match the object sizes in your dataset.\n",
        "4. **Splitting Data**: Dividing the dataset into training, validation, and test sets.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **What is the significance of anchor boxes in object detection models like YOLOv9?**\n",
        "\n",
        "**Anchor boxes** are predefined bounding boxes used in object detection models like YOLO to improve accuracy. They allow the model to predict bounding boxes of various aspect ratios and scales. During training, each ground truth bounding box is assigned to the anchor box with the highest IoU (Intersection over Union), making it easier for the model to predict accurate bounding boxes.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **What is the key difference between YOLO and R-CNN architectures?**\n",
        "\n",
        "- **YOLO**: It is a single-shot detector. The entire image is processed at once, predicting multiple bounding boxes and class labels in one forward pass.\n",
        "- **R-CNN**: It uses region proposal networks or selective search to generate candidate regions, then applies a CNN for feature extraction and classification on each region, making it slower and more computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Why is Faster RCNN considered faster than Fast RCNN?**\n",
        "\n",
        "Faster RCNN is considered faster than Fast RCNN because it replaces the slower region proposal step (which is done by selective search in Fast RCNN) with the Region Proposal Network (RPN). RPN directly generates region proposals from the convolutional feature map, eliminating the need for an external region proposal algorithm, thus speeding up the process.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. **What is the role of selective search in RCNN?**\n",
        "\n",
        "**Selective Search** is used in traditional R-CNN to generate region proposals. It is a computationally expensive algorithm that uses image segmentation and hierarchical grouping of similar regions to identify potential object locations in an image. However, itâ€™s slow and has been replaced by RPNs in Faster RCNN for better speed.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. **How does YOLOv9 handle multiple classes in object detection?**\n",
        "\n",
        "YOLOv9 handles multiple classes by dividing the image into a grid and predicting multiple bounding boxes per grid cell. Each bounding box is associated with a class probability distribution, which is used to identify the object type. This way, YOLO can handle multi-class detection within each grid cell.\n",
        "\n",
        "---\n",
        "\n",
        "### 13. **What are the key differences between YOLOv3 and YOLOv9?**\n",
        "\n",
        "While YOLOv9 is not officially released, based on previous iterations, the key improvements that could differentiate YOLOv9 from YOLOv3 include:\n",
        "- **Improved architecture**: Possibly incorporating more advanced feature extraction techniques or improved backbone networks.\n",
        "- **Higher accuracy**: Better detection of small objects and improved class prediction accuracy.\n",
        "- **Optimized speed**: Through architectural adjustments like more efficient convolutional operations, possibly allowing real-time detection at higher resolutions.\n",
        "\n",
        "---\n",
        "\n",
        "### 14. **How is the loss function calculated in Faster RCNN?**\n",
        "\n",
        "In Faster RCNN, the loss function consists of two main components:\n",
        "1. **Classification loss**: The cross-entropy loss for classifying the objects in the region proposals.\n",
        "2. **Bounding box regression loss**: The smooth L1 loss used for refining the coordinates of the bounding boxes to minimize the difference between predicted and ground truth bounding boxes.\n",
        "\n",
        "The final loss is the sum of both classification and bounding box losses.\n",
        "\n",
        "---\n",
        "\n",
        "### 15. **Explain how YOLOv9 improves speed compared to earlier versions.**\n",
        "\n",
        "Possible improvements in speed in YOLOv9 (as speculated in future versions) could include:\n",
        "- **Efficient network architecture**: Use of lightweight backbones (e.g., CSPDarknet, EfficientNet) that reduce computational complexity without sacrificing accuracy.\n",
        "- **Optimization techniques**: Leveraging model pruning, quantization, and distillation to reduce the size and improve inference time.\n",
        "- **Better anchor box optimization**: More effective anchor box strategies for quicker convergence and better object localization.\n",
        "\n",
        "### 16. **What are some challenges faced in training YOLOv9?**\n",
        "\n",
        "Training YOLOv9, like other deep learning models, comes with several challenges:\n",
        "- **Large Datasets**: YOLO requires a vast amount of annotated data to perform well, especially for detecting a wide variety of objects in diverse conditions.\n",
        "- **Class Imbalance**: Object detection datasets often have imbalanced class distributions, which can cause the model to underperform on less-represented classes.\n",
        "- **Small Object Detection**: YOLO has traditionally struggled with detecting small objects due to its grid-based approach, and fine-tuning the architecture to improve small object detection remains a challenge.\n",
        "- **Training Time and Hardware**: YOLOv9 may require significant computational resources to train effectively, especially if training on high-resolution images or large datasets.\n",
        "- **Hyperparameter Tuning**: Finding the optimal set of hyperparameters (e.g., learning rate, batch size) to ensure good performance can be tricky.\n",
        "\n",
        "---\n",
        "\n",
        "### 17. **How does the YOLOv9 architecture handle large and small object detection?**\n",
        "\n",
        "- **Large Object Detection**: YOLOv9 likely incorporates multiple scales and more refined feature maps, allowing it to detect large objects with high accuracy.\n",
        "- **Small Object Detection**: The challenge for small objects is often handled by using more anchor boxes at different scales and increasing the resolution of the feature maps. Some YOLO versions use **feature pyramid networks (FPN)** to improve the detection of small objects by merging features from multiple layers.\n",
        "\n",
        "---\n",
        "\n",
        "### 18. **What is the significance of fine-tuning in YOLO?**\n",
        "\n",
        "**Fine-tuning** is the process of adapting a pre-trained model to a new task by continuing training on a smaller, task-specific dataset. Fine-tuning in YOLO is important because:\n",
        "- It leverages pre-learned features from large datasets (like COCO or ImageNet), saving time and computational resources.\n",
        "- Fine-tuning allows the model to specialize for the specific objects and conditions in a custom dataset, improving accuracy on that dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 19. **What is the concept of bounding box regression in Faster RCNN?**\n",
        "\n",
        "**Bounding box regression** refers to the process of predicting the coordinates of bounding boxes (the location of detected objects) relative to the region proposals. In Faster RCNN, after the Region Proposal Network (RPN) generates proposals, bounding box regression is used to refine these proposals so that the predicted bounding boxes more accurately match the ground truth.\n",
        "\n",
        "---\n",
        "\n",
        "### 20. **Describe how transfer learning is used in YOLO.**\n",
        "\n",
        "**Transfer learning** in YOLO is used to accelerate training and improve performance, especially when there is limited data. The model is initially trained on a large, general-purpose dataset (e.g., COCO), and then fine-tuned on a smaller, domain-specific dataset. This allows YOLO to leverage learned features from a wide variety of objects to detect objects in a new dataset more effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### 21. **What is the role of the backbone network in object detection models like YOLOv9?**\n",
        "\n",
        "The **backbone network** is responsible for feature extraction in object detection models like YOLOv9. It takes the input image and extracts relevant features (e.g., edges, textures, shapes) at various levels of abstraction using convolutional layers. For YOLOv9, the backbone might be an advanced network like **CSPDarknet** or **EfficientNet**, which balances performance with computational efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### 22. **How does YOLO handle overlapping objects?**\n",
        "\n",
        "YOLO handles overlapping objects through a technique called **Non-Maximum Suppression (NMS)**. When YOLO predicts multiple bounding boxes for a single object, NMS is used to eliminate the redundant boxes. It keeps the bounding box with the highest confidence score and removes others that have significant overlap (IoU > threshold).\n",
        "\n",
        "---\n",
        "\n",
        "### 23. **What is the importance of data augmentation in object detection?**\n",
        "\n",
        "**Data augmentation** is crucial for training robust object detection models. It helps:\n",
        "- **Improve generalization**: By artificially increasing the diversity of the training set through transformations like rotation, flipping, scaling, and color changes, the model can learn to handle real-world variations in object appearance and positioning.\n",
        "- **Prevent overfitting**: More training examples (even synthetic ones) reduce the risk of overfitting on a small training set.\n",
        "\n",
        "---\n",
        "\n",
        "### 24. **How is performance evaluated in YOLO-based object detection?**\n",
        "\n",
        "YOLO's performance is typically evaluated using the following metrics:\n",
        "- **Mean Average Precision (mAP)**: Measures the average precision across multiple IoU thresholds, giving a general sense of how well the model detects objects across all classes.\n",
        "- **IoU (Intersection over Union)**: A metric that compares the predicted bounding box with the ground truth box.\n",
        "- **Recall and Precision**: Recall measures how many true objects are detected, while precision measures how many of the detected objects are correct.\n",
        "\n",
        "---\n",
        "\n",
        "### 25. **How do the computational requirements of Faster RCNN compare to those of YOLO?**\n",
        "\n",
        "- **YOLO**: Known for its speed and real-time performance, YOLO typically has lower computational requirements than Faster RCNN. Since it processes the entire image in a single pass (single-stage), it can run faster, especially on resource-constrained devices like GPUs and mobile devices.\n",
        "\n",
        "- **Faster RCNN**: Although Faster RCNN is faster than the original R-CNN models, it still has relatively high computational requirements due to its two-stage detection process (RPN for proposals and CNN for classification). This can make it slower compared to YOLO, especially for real-time applications.\n",
        "\n",
        "---\n",
        "\n",
        "### 26. **What role do convolutional layers play in object detection with RCNN?**\n",
        "\n",
        "**Convolutional layers** in RCNN (and its variants) are responsible for extracting features from the input image. These features (like edges, textures, and shapes) are used by subsequent layers for object localization and classification. In Fast RCNN and Faster RCNN, the convolutional layers help to generate feature maps, which are then used for region proposals (RPN in Faster RCNN) and classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### 27. **How does the loss function in YOLO differ from other object detection models?**\n",
        "\n",
        "YOLOâ€™s loss function combines:\n",
        "- **Localization loss**: A measure of how far the predicted bounding boxes are from the ground truth.\n",
        "- **Confidence loss**: How confident the model is in the predictions (whether the object is present or not).\n",
        "- **Classification loss**: How well the predicted class matches the true class.\n",
        "\n",
        "Compared to models like Faster RCNN, YOLO uses a **single loss function** that combines these factors and operates in one pass, which contributes to its faster performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 28. **What are the key advantages of using YOLO for real-time object detection?**\n",
        "\n",
        "- **Speed**: YOLO processes the entire image in one pass, making it extremely fast and suitable for real-time applications.\n",
        "- **Efficiency**: YOLOâ€™s grid-based approach makes it computationally efficient, enabling deployment on devices with limited computational power.\n",
        "- **Single-stage architecture**: Unlike two-stage models like Faster RCNN, YOLO predicts bounding boxes and class probabilities in one step, reducing complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### 29. **How does Faster RCNN handle the trade-off between accuracy and speed?**\n",
        "\n",
        "**Faster RCNN** strikes a balance by using a Region Proposal Network (RPN) to generate proposals directly from feature maps, which speeds up the proposal generation process compared to methods like selective search. However, the two-stage process (RPN for proposals and CNN for classification) still makes it slower than YOLO, but it offers higher accuracy, especially for challenging object detection tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### 30. **What is the role of the backbone network in both YOLO and Faster RCNN, and how do they differ?**\n",
        "\n",
        "- **YOLO**: The backbone network in YOLO (e.g., **Darknet**) is used to extract features from the input image. YOLO typically uses lightweight, optimized backbones for fast inference, especially in real-time applications.\n",
        "\n",
        "- **Faster RCNN**: The backbone (e.g., **VGG16** or **ResNet**) in Faster RCNN is also used for feature extraction, but the overall architecture tends to be heavier and more focused on high accuracy, often at the cost of speed.\n",
        "\n",
        "The main difference lies in the backboneâ€™s role in the respective frameworks: in YOLO, it is integrated with the rest of the network to quickly process the image, while in Faster RCNN, it feeds into a two-stage pipeline (RPN + CNN) to refine predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Practical"
      ],
      "metadata": {
        "id": "0Dpu1V1CjgkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. How do you load and run inference on a custom image using the YOLOv8 model (labeled as YOLOv9)?"
      ],
      "metadata": {
        "id": "2C_g6qf7jid2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the pre-trained YOLOv8 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')  # Adjust for YOLOv8 if available\n",
        "\n",
        "# Load the custom image\n",
        "img = Image.open('path_to_your_image.jpg')\n",
        "\n",
        "# Perform inference\n",
        "results = model(img)\n",
        "\n",
        "# Display results (bounding boxes, labels, and confidence)\n",
        "results.show()  # Show the image with bounding boxes and labels\n"
      ],
      "metadata": {
        "id": "ns17ndPmjj-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. How do you load the Faster RCNN model with a ResNet50 backbone and print its architecture?"
      ],
      "metadata": {
        "id": "aFMKQxIFjj7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "# Load the pre-trained Faster RCNN model with ResNet50 backbone\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Print the architecture of the model\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "xvGrJ2RcjjsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. How do you perform inference on an online image using the Faster RCNN model and print the predictions?"
      ],
      "metadata": {
        "id": "0V2ZJRQ9jjpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# Load Faster RCNN with pre-trained ResNet50 backbone\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Fetch an online image\n",
        "url = 'https://example.com/your_image.jpg'\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Preprocess the image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Apply the transformation\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    prediction = model(img_tensor)\n",
        "\n",
        "# Print the predictions (bounding boxes, labels, and scores)\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "id": "PqHKMf5njjmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. How do you load an image and perform inference using YOLOv9, then display the detected objects with bounding boxes and class labels?"
      ],
      "metadata": {
        "id": "mMvnK5dMjjjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the YOLOv9 model (or YOLOv5 as a placeholder)\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')  # Use appropriate model for YOLOv9\n",
        "\n",
        "# Load the image\n",
        "img = Image.open('path_to_your_image.jpg')\n",
        "\n",
        "# Run inference\n",
        "results = model(img)\n",
        "\n",
        "# Display the results\n",
        "results.show()  # This will display the image with bounding boxes and class labels\n"
      ],
      "metadata": {
        "id": "lEDk9Mg9jjfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. How do you display bounding boxes for the detected objects in an image using Faster RCNN?"
      ],
      "metadata": {
        "id": "NvQvbGoVjjcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Load Faster RCNN with pre-trained ResNet50 backbone\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Load the image\n",
        "img = Image.open('path_to_your_image.jpg')\n",
        "\n",
        "# Preprocess the image\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    prediction = model(img_tensor)\n",
        "\n",
        "# Draw the bounding boxes on the image\n",
        "draw = ImageDraw.Draw(img)\n",
        "for element in range(len(prediction[0]['boxes'])):\n",
        "    box = prediction[0]['boxes'][element].tolist()\n",
        "    draw.rectangle(box, outline=\"red\", width=3)\n",
        "\n",
        "# Display the image\n",
        "img.show()\n"
      ],
      "metadata": {
        "id": "bVV6EXtwjjYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. How do you perform inference on a local image using Faster RCNN?"
      ],
      "metadata": {
        "id": "kIK3i0u3jjPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# Load the pre-trained Faster RCNN model\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Load the image\n",
        "img = Image.open('path_to_your_image.jpg')\n",
        "\n",
        "# Preprocess the image\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    prediction = model(img_tensor)\n",
        "\n",
        "# Print predictions\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "id": "prnj03cAkKE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. How can you change the confidence threshold for YOLO object detection and filter out low-confidence predictions?"
      ],
      "metadata": {
        "id": "kWeIJT6UkKBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load YOLO model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')  # Adjust for YOLOv9 if available\n",
        "\n",
        "# Perform inference with a custom confidence threshold (e.g., 0.5)\n",
        "results = model('path_to_your_image.jpg', conf_thres=0.5)  # Filter out predictions with a confidence < 0.5\n",
        "\n",
        "# Display results\n",
        "results.show()\n"
      ],
      "metadata": {
        "id": "HM6GMNvskJ81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. How do you plot the training and validation loss curves for model evaluation?"
      ],
      "metadata": {
        "id": "AHWzlZ_NkJ39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: Assuming you have lists of loss values from training and validation\n",
        "train_loss = [0.8, 0.6, 0.5, 0.3]\n",
        "val_loss = [0.9, 0.7, 0.6, 0.5]\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IoD3gm6AkJts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. How do you perform inference on multiple images from a local folder using Faster RCNN and display the bounding boxes for each?"
      ],
      "metadata": {
        "id": "HsO2TjVfjiy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Load Faster RCNN with pre-trained ResNet50 backbone\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Directory with images\n",
        "image_folder = 'path_to_your_folder/'\n",
        "\n",
        "# Loop through images\n",
        "for image_name in os.listdir(image_folder):\n",
        "    if image_name.endswith('.jpg') or image_name.endswith('.png'):\n",
        "        img_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        # Preprocess image\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "        # Perform inference\n",
        "        with torch.no_grad():\n",
        "            prediction = model(img_tensor)\n",
        "\n",
        "        # Draw bounding boxes\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        for element in range(len(prediction[0]['boxes'])):\n",
        "            box = prediction[0]['boxes'][element].tolist()\n",
        "            draw.rectangle(box, outline=\"red\", width=3)\n",
        "\n",
        "        # Show the image with bounding boxes\n",
        "        img.show()\n"
      ],
      "metadata": {
        "id": "K2CAKzqPkZAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. How do you visualize the confidence scores alongside the bounding boxes for detected objects using Faster RCNN?"
      ],
      "metadata": {
        "id": "jQLGm8PFkY1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# Load Faster RCNN model\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Load the image\n",
        "img = Image.open('path_to_your_image.jpg')\n",
        "\n",
        "# Preprocess the image\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    prediction = model(img_tensor)\n",
        "\n",
        "# Draw bounding boxes and confidence scores\n",
        "draw = ImageDraw.Draw(img)\n",
        "for i, box in enumerate(prediction[0]['boxes']):\n",
        "    score = prediction[0]['scores'][i].item()\n",
        "    if score > 0.5:  # Confidence threshold\n",
        "        box = box.tolist()\n",
        "        draw.rectangle(box, outline=\"red\", width=3)\n",
        "        draw.text((box[0], box[1]), f'{score:.2f}', fill=\"red\")\n",
        "\n",
        "# Display image\n",
        "img.show()\n"
      ],
      "metadata": {
        "id": "CyXG6d6VkeyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. How can you save the inference results (with bounding boxes) as a new image after performing detection using YOLO?"
      ],
      "metadata": {
        "id": "mOs8RHYdkj1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Load YOLO model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n",
        "\n",
        "# Load the image\n",
        "img = Image.open('path_to_your_image.jpg')\n",
        "\n",
        "# Run inference\n",
        "results = model(img)\n",
        "\n",
        "# Save the image with bounding boxes\n",
        "results.save('path_to_save_image.jpg')  # Saves the image with bounding boxes\n"
      ],
      "metadata": {
        "id": "0_6MDH-jkjsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}