{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "###Theoretical Questions"
      ],
      "metadata": {
        "id": "wWMSAZIRKXLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPRKLdm6JOOE"
      },
      "outputs": [],
      "source": [
        "#Q1. What is Deep Learning, and how is it connected to Artificial Intelligence (AI)?\n",
        "\n",
        "Deep learning is a subset of machine learning, which in turn is a subset of artificial intelligence (AI). It involves training artificial neural networks to automatically learn from large amounts of data. Deep learning models, especially deep neural networks, are designed to simulate the human brain's structure and functioning to process and learn from data in a way that enables them to make decisions or predictions.\n",
        "\n",
        "#Q2. What is a Neural Network, and What Are the Different Types?\n",
        "\n",
        "A **neural network** is a computational model inspired by the way biological neural networks in the human brain work. It consists of layers of interconnected nodes (neurons) that process input data, transform it through various mathematical operations, and output results.\n",
        "\n",
        "Some common types of neural networks include:\n",
        "- **Feedforward Neural Networks (FNNs)**: The simplest type, where data moves in one direction from input to output.\n",
        "- **Convolutional Neural Networks (CNNs)**: Primarily used for image processing tasks, as they include convolutional layers for feature extraction.\n",
        "- **Recurrent Neural Networks (RNNs)**: Suitable for sequential data, such as time series or text, since they have loops that allow them to maintain a \"memory\" of previous inputs.\n",
        "- **Generative Adversarial Networks (GANs)**: A type of neural network where two networks (generator and discriminator) compete with each other to improve the generation of data.\n",
        "- **Transformers**: Used for sequential data tasks (like NLP), these models are particularly effective in understanding the relationship between different parts of the input data.\n",
        "\n",
        "#Q3. What is the Mathematical Structure of a Neural Network?\n",
        "\n",
        "The structure of a neural network is based on layers of neurons, where each layer consists of weights, biases, and activation functions. Mathematically, a neural network can be represented as a set of matrix operations that describe how input data is transformed as it passes through the network. Specifically:\n",
        "- **Input Layer**: Receives the input features.\n",
        "- **Hidden Layers**: Intermediate layers that perform transformations on the data using weights and biases.\n",
        "- **Output Layer**: Produces the final result or prediction.\n",
        "\n",
        "For each neuron in the network, the output is calculated as:\n",
        "\\[ \\text{Output} = \\text{Activation Function}(\\sum (\\text{Input} \\times \\text{Weight}) + \\text{Bias}) \\]\n",
        "\n",
        "#Q4. What is an Activation Function, and Why is It Essential in Neural Networks?\n",
        "\n",
        "An **activation function** is a mathematical function that determines whether a neuron should be activated or not. It introduces non-linearity to the neural network, which is essential because it allows the network to learn complex patterns. Without activation functions, a neural network would essentially be a linear model, limiting its capacity to capture complex relationships.\n",
        "\n",
        "#Q5. Some Common Activation Functions:\n",
        "- **Sigmoid**: Outputs values between 0 and 1. It's often used in binary classification.\n",
        "- **Tanh (Hyperbolic Tangent)**: Outputs values between -1 and 1.\n",
        "- **ReLU (Rectified Linear Unit)**: Outputs the input directly if positive, otherwise 0. It's the most commonly used activation function.\n",
        "- **Leaky ReLU**: Similar to ReLU, but allows a small, non-zero gradient when the input is less than 0.\n",
        "- **Softmax**: Often used in the output layer for multi-class classification, it outputs probabilities.\n",
        "\n",
        "#Q6. What is a Multilayer Neural Network?\n",
        "\n",
        "A **multilayer neural network** consists of multiple hidden layers between the input and output layers. Each hidden layer performs transformations on the input data, allowing the network to learn increasingly abstract features of the data. These networks are also known as **deep neural networks (DNNs)** when they contain many hidden layers.\n",
        "\n",
        "#Q7. What is a Loss Function, and Why is It Crucial for Neural Network Training?\n",
        "\n",
        "A **loss function** is a mathematical function that measures how well the neural network's predictions align with the actual target values. During training, the goal is to minimize the loss function to improve the model's accuracy. The loss function quantifies the error, and the optimizer uses this error to adjust the weights in the network.\n",
        "\n",
        "#Q8. Common Types of Loss Functions:\n",
        "- **Mean Squared Error (MSE)**: Commonly used for regression tasks.\n",
        "- **Cross-Entropy Loss**: Typically used for classification tasks.\n",
        "- **Hinge Loss**: Often used in support vector machines (SVMs) and for classification tasks.\n",
        "\n",
        "#Q9. How Does a Neural Network Learn?\n",
        "\n",
        "A neural network learns through a process called **backpropagation**. It involves adjusting the weights of the network based on the error (loss) computed from the output. This is achieved through gradient descent, where the optimizer adjusts the weights by calculating the gradients of the loss function with respect to the weights.\n",
        "\n",
        "#Q10. What is an Optimizer in Neural Networks, and Why is It Necessary?\n",
        "\n",
        "An **optimizer** is an algorithm used to adjust the weights of the neural network to minimize the loss function. It's crucial because it determines how the weights are updated during training. Without an optimizer, the network would not learn from the data.\n",
        "\n",
        "#Q11. Common Optimizers:\n",
        "- **Stochastic Gradient Descent (SGD)**: Updates weights using the gradient of the loss function.\n",
        "- **Adam (Adaptive Moment Estimation)**: A more advanced optimizer that adapts the learning rate based on the gradients' past behavior.\n",
        "- **RMSprop**: A modification of SGD that adjusts the learning rate based on recent gradient magnitudes.\n",
        "\n",
        "#Q12. Forward and Backward Propagation in a Neural Network:\n",
        "\n",
        "- **Forward Propagation**: The process where the input data is passed through the network, layer by layer, to generate the output. The input is transformed by weights, biases, and activation functions.\n",
        "- **Backward Propagation**: After calculating the loss, the error is propagated backward through the network to adjust the weights. This is done by computing gradients of the loss function with respect to the weights and updating them accordingly.\n",
        "\n",
        "#Q13. What is Weight Initialization, and How Does It Impact Training?\n",
        "\n",
        "**Weight initialization** is the process of setting the initial values of the weights in a neural network before training. Proper initialization is critical to avoid issues like vanishing or exploding gradients. Common initialization methods include:\n",
        "- **Xavier Initialization**: Scales the weights to keep the variance of the activations across layers balanced.\n",
        "- **He Initialization**: Designed to work well with ReLU activation functions, it initializes the weights with a larger variance.\n",
        "\n",
        "#Q14. What is the Vanishing Gradient Problem in Deep Learning?\n",
        "\n",
        "The **vanishing gradient problem** occurs when the gradients used in backpropagation become extremely small, making it difficult for the model to learn effectively. This issue typically arises with deep networks and certain activation functions, such as sigmoid or tanh, where gradients shrink as they propagate back through the layers.\n",
        "\n",
        "#Q15. What is the Exploding Gradient Problem?\n",
        "\n",
        "The **exploding gradient problem** happens when the gradients become excessively large during backpropagation. This can lead to unstable weight updates, causing the network to fail to converge. This is more common with deep networks or poor weight initialization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Practical Questions"
      ],
      "metadata": {
        "id": "z8rLImL3KTaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1. How do you create a simple perceptron for basic binary classification?"
      ],
      "metadata": {
        "id": "USe8FYocJ4kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an input layer with 2 features (input size), 1 output (binary classification), and a sigmoid activation\n",
        "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an optimizer like SGD\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with some example data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
      ],
      "metadata": {
        "id": "kLptmb2XKIjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. How can you build a neural network with one hidden layer using Keras?"
      ],
      "metadata": {
        "id": "UsDuCeB6KJyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Create a neural network model with one hidden layer\n",
        "model = Sequential()\n",
        "\n",
        "# Add an input layer (input_dim=2) and a hidden layer with 8 neurons\n",
        "model.add(Dense(8, input_dim=2, activation='relu'))\n",
        "\n",
        "# Add an output layer with 1 neuron for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "7G1rMmZVKb5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras?"
      ],
      "metadata": {
        "id": "NFCwTldlKecO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.initializers import GlorotUniform\n",
        "\n",
        "# Create a simple model with Xavier initialization\n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2, activation='relu', kernel_initializer=GlorotUniform()))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "MJABoSpyKjEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. How can you apply different activation functions in a neural network in Keras?"
      ],
      "metadata": {
        "id": "PGZ5fjz6KlzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Create a model with various activation functions\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer with ReLU activation\n",
        "model.add(Dense(8, input_dim=2, activation='relu'))\n",
        "\n",
        "# Hidden layer with Tanh activation\n",
        "model.add(Dense(8, activation='tanh'))\n",
        "\n",
        "# Output layer with Sigmoid activation (for binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "HDKAFefjKp-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. How do you add dropout to a neural network model to prevent overfitting?"
      ],
      "metadata": {
        "id": "iIiGb3JBKsSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Create a model with dropout\n",
        "model = Sequential()\n",
        "\n",
        "# Add a hidden layer with ReLU activation and dropout\n",
        "model.add(Dense(8, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Drop 50% of the neurons\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "kxhAubRtKxCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. How do you manually implement forward propagation in a simple neural network?"
      ],
      "metadata": {
        "id": "1-HOowiyKy_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example input and weights\n",
        "X = np.array([[0.1, 0.2], [0.4, 0.5]])  # 2 samples, 2 features\n",
        "W = np.array([[0.3, 0.1], [0.2, 0.5]])  # 2 features, 2 neurons in hidden layer\n",
        "b = np.array([0.1, 0.2])  # Bias for the hidden layer\n",
        "\n",
        "# Forward pass through the hidden layer (no activation function yet)\n",
        "Z = np.dot(X, W) + b  # Linear transformation\n",
        "\n",
        "# Apply ReLU activation function\n",
        "A = np.maximum(0, Z)  # ReLU activation\n",
        "\n",
        "print(\"Output after forward propagation:\", A)\n"
      ],
      "metadata": {
        "id": "N83cbD-6KyyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. How do you add batch normalization to a neural network model in Keras?"
      ],
      "metadata": {
        "id": "q_cggT9IK5nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "\n",
        "# Create a model with batch normalization\n",
        "model = Sequential()\n",
        "\n",
        "# Add a hidden layer\n",
        "model.add(Dense(8, input_dim=2, activation='relu'))\n",
        "\n",
        "# Add Batch Normalization after the hidden layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "ue7QKLoeK5ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. How can you visualize the training process with accuracy and loss curves?"
      ],
      "metadata": {
        "id": "3C6KmOhqK5cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Plot loss curve\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot accuracy curve\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mflSEZ0WK5W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?"
      ],
      "metadata": {
        "id": "b2eWFz0uLGTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# Create an Adam optimizer with gradient clipping\n",
        "optimizer = Adam(clipvalue=1.0)  # Clip gradients with a value of 1.0\n",
        "\n",
        "# Compile the model with the optimizer\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "OocEK9VKK5Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10. How can you create a custom loss function in Keras?"
      ],
      "metadata": {
        "id": "jNaGtUJZK5Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true - y_pred))  # Mean squared error as an example\n",
        "\n",
        "# Use the custom loss function when compiling the model\n",
        "model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "4XDXHN2_LRQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11. How can you visualize the structure of a neural network model in Keras?"
      ],
      "metadata": {
        "id": "nMoitVqgLVEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "# Visualize the model structure\n",
        "plot_model(model, to_file='model_structure.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "XoTulYBuLXTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}